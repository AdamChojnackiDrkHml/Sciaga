# Work in Progress

Mamy algorytm $LeaderElection$, chcemy zbadać wartość oczekiwaną liczby powtórzonych rund.

Oznaczmy:
- $R_n$ - liczba powtórzonych rund, jeśli zaczynamy z $n$ kandydatami
- $p_{n,k}$ - prawdopodobieństwo, że przy $n$ kandydatach w danej rundzie, w następnej rundzie zostanie ich $k$ ($n- k$ wyrzuciło reszkę). 
- Jeśli $k = 0$, to powtarzamy rundę. _*De facto*_ to samo dzieje się dla $k = n$, ale z polecenia wynika, że interesują nas tylko same reszki. 

Zauważmy, że:

$$
\begin{align*}
    R_1 &= 0 \\
        R_2 &= 
\begin{cases}
    R_{2} + 1 & \text{ dla 2 reszek} \\
    R_{1} & \text{ dla 1 reszki}\\
    R_{2} & \text{ dla 0 reszek } \\
\end{cases}\\
    R_n &= 
\begin{cases}
    R_{n} + 1 & \text{ dla $n$ reszek} \\
    R_{1} & \text{ dla $n - 1$ reszek}\\
    \;\;\;\;\;\vdots \\
    R_{n - 1} & \text{ dla 1 reszki } \\
    R_{n} & \text{ dla 0 reszek } \\
\end{cases}
\end{align*}
$$

No ok, wszystko ładnie, ale ile wynosi $p_{n,k}$? Jest to prawdopodobieństwo, że $k$ graczy wyrzuciło orła, $n - k$ reszkę, a więc:

$$
\begin{align*}
    p_{n,k} &= \binom{n}{k}(\frac{1}{2})^k(1 - \frac{1}{2})^{n-k}\\
            &= \binom{n}{k}(\frac{1}{2})^n
\end{align*}
$$

Sprawdżmy sobie $n = 2$.
$$
\begin{align*}
    E[R_2] &= \frac{1}{4}(E[R_2] + 1) + \frac{1}{2}E[R_1] + \frac{1}{4}E[R_2] \implies R_2 = \frac{1}{2}
\end{align*}
$$

No i fajno, teraz możemy już policzyć wartość oczekiwaną

$$
\begin{align*}
    E[R_n] &= p_{n,0} (E[R_n] + 1) + p_{n,1} E[R_1] + p_{n,2} E[R_2] + \dots + p_{n,n} E[R_n] \\ 
        &= \sum\limits_{k = 0}^{n} p_{n,k} E[R_k] + p_{n,0} \text{ // Niech $R_0 = R_n$, żeby się nie męczyć}\\
        &= (*)
\end{align*}
$$

Mamy więc rekursję i może nie jest do końca oczywiste, co z nią zrobić. Spróbujmy wyłączyć $R_n$ i $R_0$ z sumy.  

$$
\begin{align*}
    (*) &= \sum\limits_{k = 1}^{n - 1} p_{n,k} E[R_k] + 2p_{n,n} E[R_n] + p_{n,0} \\
        &= \sum\limits_{k = 1}^{n - 1} \frac{1}{2}(p_{n-1,k} + p_{n-1,k-1} )E[R_k] + 2p_{n,n} E[R_n] + p_{n,0} \\
        &= \frac{1}{2} \left(\sum\limits_{k = 0}^{n - 1} (p_{n-1,k} + p_{n-1,k-1} )E[R_k] - (p_{n-1,0} + p_{n-1,-1})E[R_{n-1}] \right) + 2p_{n,n} E[R_n] + p_{n,0} \\
        &= \frac{1}{2} \left(\sum\limits_{k = 0}^{n - 1} (p_{n-1,k} + p_{n-1,k-1} )E[R_k] - p_{n-1,0}E[R_{n-1}] \right) + 2p_{n,n} E[R_n] + p_{n,0} \\
        &= \frac{1}{2} \left( E[R_{n-1}] + \sum\limits_{k = 0}^{n - 1} p_{n-1,k-1}E[R_k] -p_{n-1,0}E[R_{n-1}]\right)+ 2p_{n,n} E[R_n] + p_{n,0} \\
        &= \frac{1}{2} \left(E[R_{n-1}] + \sum\limits_{k = 0}^{n - 1} p_{n-1,k-1}\left(\sum\limits_{j = 0}^{k} p_{k,j} E[R_j] + p_{k,0} \right) -p_{n-1,0}E[R_{n-1}]\right)+ 2p_{n,n} E[R_n] + p_{n,0} \\
\end{align*}
$$

Ok, to wygląda strasznie. Może inny pomysł - weżmy $r_n = E[R_n]$ i $r_{n-1} = E[R_{n-1}]$ i odejmijmy od siebie. 


$$
\begin{align*}
    r_n &= \sum\limits_{k = 0}^{n} p_{n,k} E[R_k] + p_{n,0}\\
    r_{n-1} &= \sum\limits_{k = 0}^{n-1} p_{n-1,k} E[R_k] + p_{n-1,0}\\
    r_n - r_{n-1} &= p_{n,n}E[R_n] + \sum\limits_{k = 0}^{n-1} \left (p_{n,k} E[R_k] - p_{n-1,k} E[R_k] \right) + p_{n,0} - p_{n-1,0}\\
    r_n - r_{n-1} &= p_{n,n}E[R_n] + \sum\limits_{k = 0}^{n-1} \left (p_{n-1,k-1} E[R_k] \right) + p_{n,0} - p_{n-1,0}
\end{align*}
$$
 W sumie też lipa, nie wiadomo, co zrobić z tym $p_{n-1,k-1}$.

Możemy też zapisać to z funckją tworzącą i to nawet trochę ładniej się prezentuje, ale dalej nie wiadomo raczej co robić:

$$
\begin{align*}
    G_n(z) &= \frac{1}{2^n} z G_n(z) + \sum\limits_{k = 0}^{n-1} p_{n,k} G_{n-k}(z)
\end{align*}
$$
